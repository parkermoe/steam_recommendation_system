{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple, Dict, Any\n",
    "from preprocessing_utils import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "user_item_path = '/Volumes/DeepLearner/Search & Recommendation System/Data/australian_users_items_clean.json'\n",
    "review_path = '/Volumes/DeepLearner/Search & Recommendation System/Data/steam_reviews_clean.json'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteamDataset(Dataset):\n",
    "    def __init__(self, X, y, user_ids, item_ids):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.user_ids = user_ids\n",
    "        self.item_ids = item_ids\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.user_ids[idx], self.item_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(user_item_path: str, review_path: str) -> Tuple[pd.DataFrame, int, int]:\n",
    "    \"\"\"\n",
    "    Load and merge user-item and review data.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_item_path (str): Path to user-item data file.\n",
    "    - review_path (str): Path to review data file.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[pd.DataFrame, int, int]: Merged DataFrame, number of unique users, and number of unique items.\n",
    "    \"\"\"\n",
    "    user_item_df = load_json_to_df(user_item_path)\n",
    "    review_df = load_review_json_to_df(review_path)\n",
    "    merged_df = pd.merge(user_item_df, review_df, how='inner', on=['user_id', 'item_id'])\n",
    "    \n",
    "    num_users = merged_df['user_id'].nunique()\n",
    "    num_items = merged_df['item_id'].nunique()\n",
    "    \n",
    "    return merged_df, num_users, num_items\n",
    "\n",
    "\n",
    "def feature_engineering(merged_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the merged DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - merged_df (pd.DataFrame): The DataFrame to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[np.ndarray, np.ndarray]: TF-IDF features and scaled target variable.\n",
    "    \"\"\"\n",
    "    # TF-IDF Vectorization for review text\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    tfidf_features = tfidf_vectorizer.fit_transform(merged_df['review']).toarray()\n",
    "    \n",
    "    # Target variable: 'playtime_forever'\n",
    "    y = merged_df['playtime_forever'].values\n",
    "    \n",
    "    # Scale the target variable\n",
    "    scaler = StandardScaler()\n",
    "    y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    return tfidf_features, y_scaled\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataloader(X, y, user_ids_array, item_ids_array, batch_size=32):\n",
    "    # Split 70% for training, 15% for validation, and 15% for testing\n",
    "    train_size = int(0.7 * len(X))\n",
    "    val_size = int(0.15 * len(X))\n",
    "    test_size = len(X) - train_size - val_size\n",
    "    \n",
    "    # Shuffle and split the data\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    \n",
    "    # Create train, val, test datasets\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_val, y_val = X[val_indices], y[val_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    \n",
    "    user_ids_train, item_ids_train = user_ids_array[train_indices], item_ids_array[train_indices]\n",
    "    user_ids_val, item_ids_val = user_ids_array[val_indices], item_ids_array[val_indices]\n",
    "    user_ids_test, item_ids_test = user_ids_array[test_indices], item_ids_array[test_indices]\n",
    "    \n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test)\n",
    "    \n",
    "    # Create datasets using the custom SteamDataset class\n",
    "    train_dataset = SteamDataset(X_train_tensor, y_train_tensor, user_ids_train, item_ids_train)\n",
    "    val_dataset = SteamDataset(X_val_tensor, y_val_tensor, user_ids_val, item_ids_val)\n",
    "    test_dataset = SteamDataset(X_test_tensor, y_test_tensor, user_ids_test, item_ids_test)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def create_item_mapping(df: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create a mapping between item IDs and item names.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing item information.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, str]: Mapping from item_id to item_name.\n",
    "    \"\"\"\n",
    "    return dict(zip(df['item_id'], df['item_name']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_text_features, embedding_dim):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        # User and Item Embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Text feature layers\n",
    "        self.text_layers = nn.Sequential(\n",
    "            nn.Linear(num_text_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined layers\n",
    "        self.combined_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2 + 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_ids, item_ids, text_features):\n",
    "        user_embedding = self.user_embedding(user_ids)\n",
    "        item_embedding = self.item_embedding(item_ids)\n",
    "        \n",
    "        text_output = self.text_layers(text_features)\n",
    "        \n",
    "        # Concatenate the embeddings and text features\n",
    "        combined_input = torch.cat([user_embedding, item_embedding, text_output], dim=1)\n",
    "        \n",
    "        output = self.combined_layers(combined_input)\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_text_features, num_numerical_features, embedding_dim):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        # Text features layer\n",
    "        self.text_layer = nn.Linear(num_text_features, 64)\n",
    "        \n",
    "        # Numerical features layer\n",
    "        self.numerical_layer = nn.Linear(num_numerical_features, 64)\n",
    "        \n",
    "        # User and Item Embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(64 + 64 + 2 * embedding_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, text_data, numerical_data, user_id, item_id):\n",
    "        # Text features\n",
    "        text_out = F.relu(self.text_layer(text_data))\n",
    "        \n",
    "        # Numerical features\n",
    "        numerical_out = F.relu(self.numerical_layer(numerical_data))\n",
    "        \n",
    "        # User and Item Embedding\n",
    "        user_embedding = self.user_embedding(user_id)\n",
    "        item_embedding = self.item_embedding(item_id)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        concat_features = torch.cat([text_out, numerical_out, user_embedding, item_embedding], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(concat_features))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        for batch_data in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            # Get batch data\n",
    "            X, y, user_id, item_id = batch_data\n",
    "            X, y, user_id, item_id = (\n",
    "                X.to(device),\n",
    "                y.to(device),\n",
    "                user_id.to(device),\n",
    "                item_id.to(device),\n",
    "                \n",
    "            )\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X, numerical_data, user_id, item_id)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss for this epoch\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                X, y, user_id, item_id = batch_data\n",
    "                X, y, user_id, item_id = (\n",
    "                    X.to(device),\n",
    "                    y.to(device),\n",
    "                    user_id.to(device),\n",
    "                    item_id.to(device),\n",
    "                    \n",
    "                )\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(text_data, numerical_data, user_id, item_id)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs.squeeze(), targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average validation loss for this epoch\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_top_k_games(user_id, user_item_df, item_mapping, k=5):\n",
    "    \"\"\"\n",
    "    Get the actual top k games for a given user based on playtime.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id (int): The ID of the user\n",
    "    - user_item_df (pd.DataFrame): DataFrame containing user-item interactions\n",
    "    - item_mapping (dict): Mapping from item_id to item_name\n",
    "    - k (int): Number of top items to return\n",
    "    \n",
    "    Returns:\n",
    "    - list: Top k items for the user based on actual playtime\n",
    "    \"\"\"\n",
    "    user_data = user_item_df[user_item_df['user_id'] == user_id]\n",
    "    top_k_items = user_data.nlargest(k, 'playtime_forever')['item_id'].tolist()\n",
    "    top_k_item_names = [item_mapping[item_id] for item_id in top_k_items]\n",
    "    \n",
    "    return top_k_item_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k_games_for_user(model, user_id, user_item_matrix, item_mapping, k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_id_tensor = torch.tensor([user_id] * len(item_mapping), dtype=torch.long)\n",
    "        item_id_tensor = torch.tensor(list(item_mapping.keys()), dtype=torch.long)\n",
    "        \n",
    "        # Dummy text_features tensor. Replace this with actual data if available.\n",
    "        text_features = torch.zeros((len(item_mapping), 5000))\n",
    "        \n",
    "        predictions = model(user_id_tensor, item_id_tensor, text_features)\n",
    "        \n",
    "    top_k_indices = predictions.argsort(descending=True)[:k]\n",
    "    top_k_item_ids = item_id_tensor[top_k_indices].tolist()\n",
    "    top_k_item_names = [item_mapping[item_id] for item_id in top_k_item_ids]\n",
    "    \n",
    "    actual_top_k_item_names = get_actual_top_k_games(user_id, user_item_matrix, item_mapping, k)\n",
    "    \n",
    "    return actual_top_k_item_names, top_k_item_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(predicted_top_k, actual_top_k, k):\n",
    "    relevant_items = set(predicted_top_k) & set(actual_top_k)\n",
    "    precision = len(relevant_items) / k\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Wrap your loader with tqdm to display the progress bar\n",
    "    for i, (X_batch, y_batch, user_ids, item_ids) in tqdm(enumerate(train_loader), desc=\"Training\", total=len(train_loader)):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        user_ids, item_ids = user_ids.long().to(device), item_ids.long().to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(user_ids, item_ids, X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    # Wrap your loader with tqdm to display the progress bar\n",
    "    with torch.no_grad():\n",
    "        for i, (X_batch, y_batch, user_ids, item_ids) in tqdm(enumerate(val_loader), desc=\"Evaluating\", total=len(val_loader)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            user_ids, item_ids = user_ids.long().to(device), item_ids.long().to(device)\n",
    "\n",
    "            \n",
    "            output = model(user_ids, item_ids, X_batch)\n",
    "            \n",
    "            y_true += list(y_batch.cpu().numpy())\n",
    "            y_pred += list(output.cpu().numpy())\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, num_users, num_items = load_and_merge_data(user_item_path, review_path)\n",
    "\n",
    "X, y = feature_engineering(merged_df)\n",
    "\n",
    "user_ids_array = merged_df['user_id'].astype('category').cat.codes.values\n",
    "item_ids_array = merged_df['item_id'].astype('category').cat.codes.values\n",
    "\n",
    "item_mapping = create_item_mapping(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, num_users, num_items = load_and_merge_data(user_item_path, review_path)\n",
    "\n",
    "X, y = feature_engineering(merged_df)\n",
    "\n",
    "user_ids_array = merged_df['user_id'].astype('category').cat.codes.values\n",
    "item_ids_array = merged_df['item_id'].astype('category').cat.codes.values\n",
    "# getting train, test, val data\n",
    "train_loader, val_loader, test_loader = prepare_dataloader(X, y, user_ids_array, item_ids_array, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|█████████▉| 1011/1014 [00:18<00:00, 63.69it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:18<00:00, 54.32it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:01<00:00, 187.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.0259\n",
      "Validation MSE: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|█████████▉| 1011/1014 [00:16<00:00, 62.32it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:16<00:00, 61.41it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 262.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 1.0215\n",
      "Validation MSE: 0.9187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|█████████▉| 1013/1014 [00:16<00:00, 63.11it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:16<00:00, 62.91it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 268.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 1.0201\n",
      "Validation MSE: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training:  99%|█████████▉| 1007/1014 [00:15<00:00, 63.45it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:15<00:00, 63.53it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 273.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 1.0228\n",
      "Validation MSE: 0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training:  99%|█████████▉| 1008/1014 [00:15<00:00, 64.85it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:15<00:00, 63.68it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 258.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 1.0200\n",
      "Validation MSE: 0.9178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|█████████▉| 1013/1014 [00:17<00:00, 62.13it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:17<00:00, 57.87it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 267.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 1.0200\n",
      "Validation MSE: 0.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training:  99%|█████████▉| 1007/1014 [00:15<00:00, 64.72it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:15<00:00, 63.75it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 267.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 1.0200\n",
      "Validation MSE: 0.9278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|█████████▉| 1013/1014 [00:16<00:00, 63.65it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:16<00:00, 60.69it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 274.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 1.0205\n",
      "Validation MSE: 0.9173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|█████████▉| 1012/1014 [00:15<00:00, 63.71it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:16<00:00, 63.35it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 267.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 1.0199\n",
      "Validation MSE: 0.9232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1014 [00:00<?, ?it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training:  99%|█████████▉| 1007/1014 [00:15<00:00, 64.67it/s]/Users/parkermoesta/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training: 100%|██████████| 1014/1014 [00:15<00:00, 63.75it/s]\n",
      "Evaluating: 100%|██████████| 218/218 [00:00<00:00, 273.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 1.0217\n",
      "Validation MSE: 0.9208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_text_features = 5000\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\")\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = HybridModel(num_users, num_items, num_text_features, embedding_dim).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Load data (assuming train_loader and val_loader are prepared)\n",
    "# train_loader, val_loader = prepare_dataloader(...)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_mse = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5000])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# check first batch of train_loader\n",
    "\n",
    "for batch_data in train_loader:\n",
    "    X, y, user_id, item_id = batch_data\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(user_id.shape)\n",
    "    print(item_id.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w_/6hq13d5j2kvcn5sylq3jc1tc0000gn/T/ipykernel_10788/2295941096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspecific_user_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m  \u001b[0;31m# Replace with an actual user ID from your data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspecific_user_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mspecific_user_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mactual_top_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_top_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_top_k_games_for_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecific_user_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_top_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_top_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "specific_user_id = 10  # Replace with an actual user ID from your data\n",
    "specific_user_code = merged_df[merged_df['user_id'] == specific_user_id]['user_id'].astype('category').cat.codes.values[0]\n",
    "\n",
    "actual_top_5, predicted_top_5 = predict_top_k_games_for_user(model, specific_user_code, merged_df, item_mapping, k=5)\n",
    "precision = precision_at_k(predicted_top_5, actual_top_5, k=5)\n",
    "\n",
    "print(f\"Actual Top 5 games: {actual_top_5}\")\n",
    "print(f\"Predicted Top 5 games: {predicted_top_5}\")\n",
    "print(f\"Precision@5: {precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
